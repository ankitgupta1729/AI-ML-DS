{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "917b5841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d2e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --disable-pip-version-check torch torchdata --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40744a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "074f60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cee2687",
   "metadata": {},
   "source": [
    "#### Summarize Dialogue without Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f271f8a",
   "metadata": {},
   "source": [
    "Here, we will be generating a summary of a dialogue with a pre-trained LLM FLAN-T5 from HuggingFace.\n",
    "Let's upload some simple dialogues from `dialogsum` huggindface dataset. This dataset contains 10k+ dialogues with the\n",
    "corresponding manually labelled summaries and topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "727537de",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = 'knkarthick/dialogsum'\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20366dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example 1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example 2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print a couple of dialogues with their baseline summaries\n",
    "example_indices = [40,200]\n",
    "dash_line = '-'.join('' for _ in range(100))\n",
    "for i,index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example',i+1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE:')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4082faa5",
   "metadata": {},
   "source": [
    "Now, we will improve the summary by our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66dfd50",
   "metadata": {},
   "source": [
    "Load the `Flan-T5` model and create an instance of the `AutoModelForSeq2SeqLM` class with `.from_pretrained()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af96c362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad2493bff62456ca9497fdeab43051e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/282 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f53517a",
   "metadata": {},
   "source": [
    "To perform encoding and decoding, you need the text in tokenized form. Tokenization is the process of splitting text\n",
    "into smaller units that can be processed by LLM models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc113e",
   "metadata": {},
   "source": [
    "Download the tokenizer for `Flan-T5` model using `AutoTokenizer.from_pretrained()` method. Parameter `use_fast` \n",
    "switches on the fast tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0d39af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e27fd",
   "metadata": {},
   "source": [
    "These are all from the huggingface transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64141198",
   "metadata": {},
   "source": [
    "#### Test the tokenizer encoding and decoding a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d71af0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Sentence: \n",
      "tensor([ 363,   97,   19,   34,    6, 3059,   58,    1])\n",
      "\n",
      " Decoded Sentence: \n",
      "What time is it, Tom?\n"
     ]
    }
   ],
   "source": [
    "sentence = 'What time is it, Tom?'\n",
    "sentence_encoded = tokenizer(sentence,return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(sentence_encoded['input_ids'][0],skip_special_tokens=True)\n",
    "print(\"Encoded Sentence: \")\n",
    "print(sentence_encoded['input_ids'][0])\n",
    "print('\\n Decoded Sentence: ')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f8f71",
   "metadata": {},
   "source": [
    "Now, it's time to explore how well the base LLM summarizes a dialogue without any prompt engineering. Prompt\n",
    "engineering is an act of human changing the prompt to improve the response for a given task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2df95f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example 1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      " Person1: It's ten to nine.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example 2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      " #Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary =  dataset['test'][index]['summary']\n",
    "    inputs = tokenizer(dialogue,return_tensors = 'pt')\n",
    "    output = tokenizer.decode(model.generate(inputs['input_ids'], max_new_tokens=50,)[0], skip_special_tokens=True)\n",
    "    print(dash_line)\n",
    "    print('Example',i+1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n {output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad88d834",
   "metadata": {},
   "source": [
    "It is not doing a very good job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7d6fe",
   "metadata": {},
   "source": [
    "#### Summarize Dialogue with an Instruction Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c07d08",
   "metadata": {},
   "source": [
    "Prompt Engineering is an important concept in using foundation models for text generation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b99ba5",
   "metadata": {},
   "source": [
    "##### Zero Shot Inference with an Instruction Prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d662768a",
   "metadata": {},
   "source": [
    "In order to instruct the model to perform a task - summarize a dialogue - you can take the dialogue and convert\n",
    "it into an instruction prompt. This is often called \"zero-shot inference\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7408f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example 1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "\n",
      "    Summarize the following conversation.\n",
      "    #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "    Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      " The train is about to leave.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example 2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "\n",
      "    Summarize the following conversation.\n",
      "    #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "    Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      " #Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary =  dataset['test'][index]['summary']\n",
    "    prompt = f\"\"\"\n",
    "    \n",
    "    Summarize the following conversation.\n",
    "    {dialogue}\n",
    "    Summary:\n",
    "    \"\"\"\n",
    "    #input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt,return_tensors = 'pt')\n",
    "    output = tokenizer.decode(model.generate(inputs['input_ids'], max_new_tokens=50,)[0], skip_special_tokens=True)\n",
    "    print(dash_line)\n",
    "    print('Example',i+1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n {output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "236289a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example 1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "\n",
      "    Dialogue:\n",
      "    #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "    What was going on?\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT WITH TEMPERATURE AND TOP-P SAMPLING:\n",
      " Tom is late for the train.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example 2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "\n",
      "    Dialogue:\n",
      "    #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "    What was going on?\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT WITH TEMPERATURE AND TOP-P SAMPLING:\n",
      " #Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary =  dataset['test'][index]['summary']\n",
    "    prompt = f\"\"\"\n",
    "    \n",
    "    Dialogue:\n",
    "    {dialogue}\n",
    "    What was going on?\n",
    "    \"\"\"\n",
    "    #input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt,return_tensors = 'pt')\n",
    "    generation_config = GenerationConfig(max_new_tokens=50, temperature=0.7, top_p=0.9)\n",
    "    output = tokenizer.decode(model.generate(inputs['input_ids'], generation_config=generation_config)[0], skip_special_tokens=True)\n",
    "    print(dash_line)\n",
    "    print('Example',i+1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - ZERO SHOT WITH TEMPERATURE AND TOP-P SAMPLING:\\n {output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd0e637",
   "metadata": {},
   "source": [
    "#### Summarize dialogue with One Shot and Few Shot Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c270ee",
   "metadata": {},
   "source": [
    "One shot and few shot inference are the practices of providing an LLM with either one or more full examples of prompt-response\n",
    "pairs that match your task before your actual prompt that you want completed. This is called \"in-context\" learning and puts\n",
    "your model into a state that understands your specific task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c2b9e",
   "metadata": {},
   "source": [
    "#####  One Shot Inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb009236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for idx in example_indices_full:\n",
    "        dialogue = dataset['test'][idx]['dialogue']\n",
    "        summary = dataset['test'][idx]['summary']\n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred\n",
    "        # stop sequence        \n",
    "        prompt += f\"\"\"\n",
    "        Dialogue:\n",
    "        {dialogue}\n",
    "        What was going on?\n",
    "        {summary}\n",
    "        \"\"\"\n",
    "    dialogue_to_summarize = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "    prompt += f\"\"\"\n",
    "    Dialogue:\n",
    "    {dialogue_to_summarize}\n",
    "    What was going on?\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfafb49",
   "metadata": {},
   "source": [
    "Construct the prompt to perform one shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eb7fd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONE SHOT PROMPT:\n",
      "\n",
      "        Dialogue:\n",
      "        #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "        What was going on?\n",
      "        #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "        \n",
      "    Dialogue:\n",
      "    #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "    What was going on?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40]\n",
    "example_index_to_summarize = 200\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "print('ONE SHOT PROMPT:')\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f4ac3",
   "metadata": {},
   "source": [
    "Now, pass the prompt to perform the one shot inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91860f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ONE SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary =  dataset['test'][example_index_to_summarize]['summary']\n",
    "inputs = tokenizer(one_shot_prompt,return_tensors = 'pt')\n",
    "output = tokenizer.decode(model.generate(inputs['input_ids'], max_new_tokens=50,)[0], skip_special_tokens=True)\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf0617",
   "metadata": {},
   "source": [
    "prompt engineering, zero shot, one shot, few shot inferences are the first step always if we have a hub of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2718f666",
   "metadata": {},
   "source": [
    "##### Few Shot Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc4d675b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEW SHOT PROMPT:\n",
      "\n",
      "        Dialogue:\n",
      "        #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "        What was going on?\n",
      "        #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "        \n",
      "        Dialogue:\n",
      "        #Person1#: May, do you mind helping me prepare for the picnic?\n",
      "#Person2#: Sure. Have you checked the weather report?\n",
      "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
      "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
      "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
      "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
      "#Person1#: All set. May, can you help me take all these things to the living room?\n",
      "#Person2#: Yes, madam.\n",
      "#Person1#: Ask Daniel to give you a hand?\n",
      "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
      "        What was going on?\n",
      "        Mom asks May to help to prepare for the picnic and May agrees.\n",
      "        \n",
      "        Dialogue:\n",
      "        #Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "        What was going on?\n",
      "        #Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "        \n",
      "    Dialogue:\n",
      "    #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "    What was going on?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# we are helping the model\n",
    "example_indices_full = [40,80,120]\n",
    "example_index_to_summarize = 200\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "print('FEW SHOT PROMPT:')\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7377c81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.7, top_p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "369035c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (818 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "#1 is ready, is interested- will introduce an optional technology as per #2, will update in his or at her request. Will add on their new business services on disc/media only; #2 already gets PC software softwares to enhance skills in his\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "inputs = tokenizer(few_shot_prompt,return_tensors = 'pt')\n",
    "output = tokenizer.decode(model.generate(inputs['input_ids'], generation_config=generation_config,)[0],   skip_special_tokens=True)\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04320d",
   "metadata": {},
   "source": [
    "In this case, few shot did not provide much of an improvement over one shot inference. And, anything above 5 or 6 shot will typically not help much, either. Also, you need to ensure that you do not exceed the model's input-context length which, in \n",
    "our case, is 512 tokens. Anything above the context length will be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07c2cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
