{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "917b5841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d2e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --disable-pip-version-check torch torchdata --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40744a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "074f60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cee2687",
   "metadata": {},
   "source": [
    "#### Summarize Dialogue without Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f271f8a",
   "metadata": {},
   "source": [
    "Here, we will be generating a summary of a dialogue with a pre-trained LLM FLAN-T5 from HuggingFace.\n",
    "Let's upload some simple dialogues from `dialogsum` huggindface dataset. This dataset contains 10k+ dialogues with the\n",
    "corresponding manually labelled summaries and topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "727537de",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = 'knkarthick/dialogsum'\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20366dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example 1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example 2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print a couple of dialogues with their baseline summaries\n",
    "example_indices = [40,200]\n",
    "dash_line = '-'.join('' for _ in range(100))\n",
    "for i,index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example',i+1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE:')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4082faa5",
   "metadata": {},
   "source": [
    "Now, we will improve the summary by our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66dfd50",
   "metadata": {},
   "source": [
    "Load the `Flan-T5` model and create an instance of the `AutoModelForSeq2SeqLM` class with `.from_pretrained()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af96c362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a36cef7a454b2eacde9bf3ef9db1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a875748ae62d4f4fa7d03a29728395d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/282 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350283bea2f94990acd42aeb64b56363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f53517a",
   "metadata": {},
   "source": [
    "To perform encoding and decoding, you need the text in tokenized form. Tokenization is the process of splitting text\n",
    "into smaller units that can be processed by LLM models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc113e",
   "metadata": {},
   "source": [
    "Download the tokenizer for `Flan-T5` model using `AutoTokenizer.from_pretrained()` method. Parameter `use_fast` \n",
    "switches on the fast tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0d39af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397cc36489944930bf965f2d873ece93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc05a0f19094fd386c3383adb443da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96be9fc125dd42efa7e38baaa6aadd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64883b32e9c4d0f929baf5376b6081a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e27fd",
   "metadata": {},
   "source": [
    "These are all from the huggingface transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64141198",
   "metadata": {},
   "source": [
    "#### Test the tokenizer encoding and decoding a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d71af0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Sentence: \n",
      "tensor([ 363,   97,   19,   34,    6, 3059,   58,    1])\n",
      "\n",
      " Decoded Sentence: \n",
      "What time is it, Tom?\n"
     ]
    }
   ],
   "source": [
    "sentence = 'What time is it, Tom?'\n",
    "sentence_encoded = tokenizer(sentence,return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(sentence_encoded['input_ids'][0],skip_special_tokens=True)\n",
    "print(\"Encoded Sentence: \")\n",
    "print(sentence_encoded['input_ids'][0])\n",
    "print('\\n Decoded Sentence: ')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f8f71",
   "metadata": {},
   "source": [
    "Now, it's time to explore how well the base LLM summarizes a dialogue without any prompt engineering. Prompt\n",
    "engineering is an act of human changing the prompt to improve the response for a given task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df95f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary =  dataset['test'][index]['summary']\n",
    "    inputs = tokenizer(dialogue,return_tensors = 'pt')\n",
    "    outputs = tokenizer.decod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7408f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
